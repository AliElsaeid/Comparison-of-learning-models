# -*- coding: utf-8 -*-
"""Advanced Machine Learning Project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nrapqaVrfkg97WFHftmVsjoY17skfqfj
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler



def print_metrics(model_name, accuracy, precision, recall, f1):
    print(f"{model_name} Metrics:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")
    print()

df = pd.read_csv("bankloan.csv")
df.info()

df=df.drop_duplicates()

X1 = df.drop('Personal.Loan', axis=1)
y1 = df['Personal.Loan']

imputer1 = SimpleImputer(strategy='mean')
X1 = pd.DataFrame(imputer1.fit_transform(X1), columns=X1.columns)

# Model Implementation
random_forest1 = RandomForestClassifier()
adaboost1 = AdaBoostClassifier()
gradient_boost1 = GradientBoostingClassifier()

# Hyperparameter Tuning
param_grid_rf1 = {'n_estimators': [50, 100, 150], 'max_depth': [None, 10, 20]}
param_grid_ab1 = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.5]}
param_grid_gb1 = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.5], 'max_depth': [3, 5, 7]}

grid_search_rf1 = GridSearchCV(random_forest1, param_grid_rf1, cv=5)
grid_search_ab1 = GridSearchCV(adaboost1, param_grid_ab1, cv=5)
grid_search_gb1 = GridSearchCV(gradient_boost1, param_grid_gb1, cv=5)

X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

grid_search_rf1.fit(X1_train, y1_train)
grid_search_ab1.fit(X1_train, y1_train)
grid_search_gb1.fit(X1_train, y1_train)

predictions_rf1 = grid_search_rf1.predict(X1_test)
predictions_ab1 = grid_search_ab1.predict(X1_test)
predictions_gb1 = grid_search_gb1.predict(X1_test)

# Metrics for the first dataset
accuracy_rf1 = accuracy_score(y1_test, predictions_rf1)
precision_rf1 = precision_score(y1_test, predictions_rf1)
recall_rf1 = recall_score(y1_test, predictions_rf1)
f1_rf1 = f1_score(y1_test, predictions_rf1)

accuracy_ab1 = accuracy_score(y1_test, predictions_ab1)
precision_ab1 = precision_score(y1_test, predictions_ab1)
recall_ab1 = recall_score(y1_test, predictions_ab1)
f1_ab1 = f1_score(y1_test, predictions_ab1)

accuracy_gb1 = accuracy_score(y1_test, predictions_gb1)
precision_gb1 = precision_score(y1_test, predictions_gb1)
recall_gb1 = recall_score(y1_test, predictions_gb1)
f1_gb1 = f1_score(y1_test, predictions_gb1)

comparison_table1 = pd.DataFrame({
    'Model': ['Random Forest', 'AdaBoost', 'Gradient Boost'],
    'Accuracy': [accuracy_rf1, accuracy_ab1, accuracy_gb1],
    'Precision': [precision_rf1, precision_ab1, precision_gb1],
    'Recall': [recall_rf1, recall_ab1, recall_gb1],
    'F1-Score': [f1_rf1, f1_ab1, f1_gb1]
})
print("Comparison Table and Metrics for the Bank Loan Dataset:")
print(comparison_table1)

# Visualization
# Bar Chart
plt.bar(comparison_table1['Model'], comparison_table1['Accuracy'])
plt.title('Accuracy Comparison for the Bank Loan Dataset')
plt.ylabel('Accuracy')
plt.show()

df2 = pd.read_csv("glasstypePrediction.csv")
df.info()

df2=df2.drop_duplicates()

# Preprocessing for the second dataset
X2 = df2.drop('Type', axis=1)
y2 = df2['Type']

imputer2 = SimpleImputer(strategy='mean')
X2 = pd.DataFrame(imputer2.fit_transform(X2), columns=X2.columns)

random_forest2 = RandomForestClassifier()
adaboost2 = AdaBoostClassifier()
gradient_boost2 = GradientBoostingClassifier()

param_grid_rf2 = {'n_estimators': [50, 100, 150], 'max_depth': [None, 10, 20]}
param_grid_ab2 = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.5]}
param_grid_gb2 = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.5], 'max_depth': [3, 5, 7]}

grid_search_rf2 = GridSearchCV(random_forest2, param_grid_rf2, cv=5)
grid_search_ab2 = GridSearchCV(adaboost2, param_grid_ab2, cv=5)
grid_search_gb2 = GridSearchCV(gradient_boost2, param_grid_gb2, cv=5)

X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)

grid_search_rf2.fit(X2_train, y2_train)

grid_search_ab2.fit(X2_train, y2_train)

grid_search_gb2.fit(X2_train, y2_train)

predictions_rf2 = grid_search_rf2.predict(X2_test)
predictions_ab2 = grid_search_ab2.predict(X2_test)
predictions_gb2 = grid_search_gb2.predict(X2_test)

accuracy_rf2 = accuracy_score(y2_test, predictions_rf2)
precision_rf2 = precision_score(y2_test, predictions_rf2, average='weighted')
recall_rf2 = recall_score(y2_test, predictions_rf2, average='weighted')
f1_rf2 = f1_score(y2_test, predictions_rf2, average='weighted')

accuracy_ab2 = accuracy_score(y2_test, predictions_ab2)
precision_ab2 = precision_score(y2_test, predictions_ab2, average='weighted')
recall_ab2 = recall_score(y2_test, predictions_ab2, average='weighted')
f1_ab2 = f1_score(y2_test, predictions_ab2, average='weighted')

accuracy_gb2 = accuracy_score(y2_test, predictions_gb2)
precision_gb2 = precision_score(y2_test, predictions_gb2, average='weighted')
recall_gb2 = recall_score(y2_test, predictions_gb2, average='weighted')
f1_gb2 = f1_score(y2_test, predictions_gb2, average='weighted')

# Comparison Table for the second dataset
comparison_table2 = pd.DataFrame({
    'Model': ['Random Forest', 'AdaBoost', 'Gradient Boost'],
    'Accuracy': [accuracy_rf2, accuracy_ab2, accuracy_gb2],
    'Precision': [precision_rf2, precision_ab2, precision_gb2],
    'Recall': [recall_rf2, recall_ab2, recall_gb2],
    'F1-Score': [f1_rf2, f1_ab2, f1_gb2]
})
print("\nComparison Table and Metrics for the Glass Type Prediction Dataset:")
print(comparison_table2)

plt.bar(comparison_table2['Model'], comparison_table2['Accuracy'])
plt.title('Accuracy Comparison for the Glass Type Prediction Dataset')
plt.ylabel('Accuracy')
plt.show()



"""# dataset 3"""

data = pd.read_csv("data_banknote_authentication.csv")

df = pd.DataFrame(data)

df.fillna(df.mean(), inplace=True)

scaler = StandardScaler()
data[['Variance_Wavelet', 'Skewness_Wavelet', 'Curtosis_Wavelet', 'Image_Entropy']] = scaler.fit_transform(data[['Variance_Wavelet', 'Skewness_Wavelet', 'Curtosis_Wavelet', 'Image_Entropy']])

X = data.drop(columns=['Class'])
y = data['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_model = RandomForestClassifier(random_state=42)
ada_model = AdaBoostClassifier(random_state=42)
gb_model = GradientBoostingClassifier(random_state=42)

rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20]}
ada_params = {'n_estimators': [50, 100, 150], 'learning_rate': [0.1, 0.5, 1.0]}
gb_params = {'n_estimators': [100, 200, 300], 'learning_rate': [0.1, 0.5, 1.0]}

rf_grid = GridSearchCV(rf_model, rf_params, cv=5)
ada_grid = GridSearchCV(ada_model, ada_params, cv=5)
gb_grid = GridSearchCV(gb_model, gb_params, cv=5)

rf_grid.fit(X_train, y_train)
ada_grid.fit(X_train, y_train)
gb_grid.fit(X_train, y_train)

print("Best parameters for Random Forest:", rf_grid.best_params_)
print("Best parameters for AdaBoost:", ada_grid.best_params_)
print("Best parameters for Gradient Boost:", gb_grid.best_params_)

rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
rf_precision = precision_score(y_test, rf_pred)
rf_recall = recall_score(y_test, rf_pred)
rf_f1 = f1_score(y_test, rf_pred)
print("Random Forest - Accuracy:", rf_accuracy, "Precision:", rf_precision, "Recall:", rf_recall, "F1-score:", rf_f1)

ada_model.fit(X_train, y_train)
ada_pred = ada_model.predict(X_test)
ada_accuracy = accuracy_score(y_test, ada_pred)
ada_precision = precision_score(y_test, ada_pred)
ada_recall = recall_score(y_test, ada_pred)
ada_f1 = f1_score(y_test, ada_pred)
print("AdaBoost - Accuracy:", ada_accuracy, "Precision:", ada_precision, "Recall:", ada_recall, "F1-score:", ada_f1)

gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
gb_precision = precision_score(y_test, gb_pred)
gb_recall = recall_score(y_test, gb_pred)
gb_f1 = f1_score(y_test, gb_pred)
print("Gradient Boost - Accuracy:", gb_accuracy, "Precision:", gb_precision, "Recall:", gb_recall, "F1-score:", gb_f1)

accuracies = {
    'Dataset': ['Dataset '],
    'Random Forest': [rf_accuracy],
    'AdaBoost': [ada_accuracy],
    'Gradient Boost': [gb_accuracy]
}

#  comparison table
comparison_df = pd.DataFrame(accuracies)
print(comparison_df)

# Bar Chart
plt.figure(figsize=(10, 6))
models = ['Random Forest', 'AdaBoost', 'Gradient Boost']
accuracy_values = [rf_accuracy, ada_accuracy, gb_accuracy]
plt.bar(models, accuracy_values, color=['blue', 'green', 'red'])
plt.xlabel('Ensemble Models')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison of Ensemble Models')
plt.ylim(0.8, 1.0)
plt.show()


best_models = {
    'Dataset 1': rf_grid.best_estimator_,
    'Dataset 2': ada_grid.best_estimator_,
    'Dataset 3': gb_grid.best_estimator_
}